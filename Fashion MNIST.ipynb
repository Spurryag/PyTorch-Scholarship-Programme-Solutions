{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the required packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the helper script\n",
    "%run helper_script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct iterator\n",
    "images, label = next(iter(testloader))\n",
    "\n",
    "# Flatten MNIST images into a 784 long vector\n",
    "images = images.view(images.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define the NN architectire\n",
    "model = nn.Sequential(nn.Linear(784, 256),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(0.2),\n",
    "                      nn.Linear(256, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(0.2),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "#Define the loss function and the optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Dropout(p=0.2)\n",
       "  (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (6): ReLU()\n",
       "  (7): Dropout(p=0.2)\n",
       "  (8): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (9): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify model structure\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropout**\n",
    "\n",
    "During training, randomly zeroes some of the elements of the input tensor with probability :attr:`p` using samples from a Bernoulli distribution. The elements to zero are randomized on every forward call.\n",
    "\n",
    "This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons as described in the paper Improving neural networks by preventing co-adaptation of feature detectors`.\n",
    "\n",
    "Furthermore, the outputs are scaled by a factor of :math:`\\frac{1}{1-p}` during training. This means that during evaluation the module simply computes an identity function.\n",
    "\n",
    "Args:\n",
    "p: probability of an element to be zeroed. Default: 0.5\n",
    "inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n",
    "\n",
    "Shape:\n",
    "- Input: `Any`. Input can be of any shape\n",
    "- Output: `Same`. Output is of the same shape as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training,dropout is used to prevent overfitting, but during inference it is important to use the entire network. So, dropout needs to be turned off during validation, testing, and whenever the network will be used to make predictions. To do this, use model.eval(). This sets the model to evaluation mode where the dropout probability is 0. Dropout can be turned back on by setting the model to train mode with model.train(). In general, the pattern for the validation loop will look like this, turn off gradients, set the model to evaluation mode, calculate the validation loss and metric, then set the model back to train mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NB:*\n",
    "\n",
    "\n",
    "loss.backward() computes dloss/dx for every parameter x which has requires_grad=True. These are accumulated into x.grad for every parameter x. In pseudo-code:\n",
    "\n",
    "x.grad += dloss/dx\n",
    "\n",
    "optimizer.step updates the value of x using the gradient x.grad. For example, the SGD optimizer performs:\n",
    "\n",
    "x += -lr * x.grad\n",
    "\n",
    "optimizer.zero_grad() clears x.grad for every parameter x in the optimizer. It’s important to call this before loss.backward(), otherwise you’ll accumulate the gradients from multiple passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20..  Training Loss: 0.000..  Test Loss: 0.460..  Test Accuracy: 0.829\n",
      "Epoch: 2/20..  Training Loss: 0.000..  Test Loss: 0.426..  Test Accuracy: 0.843\n",
      "Epoch: 3/20..  Training Loss: 0.000..  Test Loss: 0.411..  Test Accuracy: 0.853\n",
      "Epoch: 4/20..  Training Loss: 0.000..  Test Loss: 0.404..  Test Accuracy: 0.856\n",
      "Epoch: 5/20..  Training Loss: 0.000..  Test Loss: 0.378..  Test Accuracy: 0.864\n",
      "Epoch: 6/20..  Training Loss: 0.000..  Test Loss: 0.397..  Test Accuracy: 0.859\n",
      "Epoch: 7/20..  Training Loss: 0.000..  Test Loss: 0.391..  Test Accuracy: 0.861\n",
      "Epoch: 8/20..  Training Loss: 0.000..  Test Loss: 0.376..  Test Accuracy: 0.866\n",
      "Epoch: 9/20..  Training Loss: 0.000..  Test Loss: 0.372..  Test Accuracy: 0.868\n"
     ]
    }
   ],
   "source": [
    "#Define the number of epochs and Steps for the optimizer\n",
    "epochs = 20\n",
    "steps = 0\n",
    "\n",
    "#Create empty list to record the training and test errors\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "#Use a 'for' loop to move through each epoch\n",
    "for e in range(epochs):\n",
    "    #The running loss is set to null at first\n",
    "    running_loss = 0\n",
    "    #For each image and their associated label in the training iterator\n",
    "    for images, labels in trainloader:\n",
    "        #Flatten the image\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        #Clear the previous gradient computations\n",
    "        optimizer.zero_grad()\n",
    "        #Given that the cross entropy function is used, raw output is the logit\n",
    "        log_ps = model(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        #run the backpropagation algorithm to update the weights\n",
    "        loss.backward()\n",
    "        #performs a parameter update based on the current gradient\n",
    "        optimizer.step()\n",
    "        #update the running error for the training data\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "            #Turn on evaluation for validation (because of dropout)\n",
    "            model.eval()\n",
    "            for images, labels in testloader:\n",
    "                #Flatten the images\n",
    "                images = images.view(images.shape[0], -1)\n",
    "                #Run the model on the above images from the test iterator\n",
    "                log_ps = model(images)\n",
    "                test_loss += criterion(log_ps, labels)\n",
    "                #Obtain probabilties from the logits obtained from the test data fit\n",
    "                ps = torch.exp(log_ps)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                #Compute the accuracy metric\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "                running_loss = 0\n",
    "            #Turn of evaluation for training(because of dropout)\n",
    "        model.train()\n",
    "                \n",
    "        #Append each result to the train and test losses        \n",
    "        train_losses.append(running_loss/len(trainloader))\n",
    "        test_losses.append(test_loss/len(testloader))\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(running_loss/len(trainloader)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader)),\n",
    "              \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "#Fplt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find when does the minimum validation error occurs \n",
    "print(\"Epoch Number: {} at Error rate: {} \".format(np.argmin(test_losses),np.min(test_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run the model evaluation\n",
    "model.eval()\n",
    "#select the testing data\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "img = images[0]\n",
    "# Convert 2D image to 1D vector\n",
    "img = img.view(1, 784)\n",
    "\n",
    "# Calculate the class probabilities (softmax) for img\n",
    "with torch.no_grad():\n",
    "    output = model.forward(img)\n",
    "#Obtain the probabilties from the logits\n",
    "ps = torch.exp(output)\n",
    "\n",
    "# Plot the image and probabilities\n",
    "view_classify(img.view(1, 28, 28), ps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
