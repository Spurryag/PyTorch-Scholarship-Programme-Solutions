{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define a transform to normalize the data - change the range of values in the image [histogram stretch]\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                              ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "#batch_size up indicates that 64 images are taken at a time from the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "#create iterator to go through images\n",
    "detaiter = iter(trainloader)\n",
    "images,labels = detaiter.next()\n",
    "print(type(images))\n",
    "print(images.shape)#64 images per batch, 1 color channel and 28 x 28 pixels\n",
    "print(labels.shape)#1 label per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot a sample image\n",
    "plt.imshow(images[1].numpy().squeeze(),cmap= 'Greys_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "        ...,\n",
      "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "        [-1., -1., -1.,  ..., -1., -1., -1.]])\n"
     ]
    }
   ],
   "source": [
    "#Need to convert the current tensor to vector such that one image is a vector of (1 * 28 * 28) or (784)\n",
    "#This will lead to each batch of 64 having size 784 => (64, 784)\n",
    "#Source: https://www.aiworkbox.com/lessons/flatten-a-pytorch-tensor\n",
    "\n",
    "flattened = images.view(images.shape[0],-1)\n",
    "print(flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Random sampling creation ops are contained in torch.randn\n",
    "#Iterator in Python is simply an object that can be iterated upon. \n",
    "#An object which will return data, one element at a time. \n",
    "\n",
    "#images.shape[0] returns the dimensions o fhte array => basically take the first batch of images and flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use sigmoid for activation layer\n",
    "\n",
    "\"\"\"\"Same as before we start by defining the activation function\"\"\"\n",
    "\n",
    "#Define the sigmoid activtion function\n",
    "def activation(x):\n",
    "    \"\"\" Sigmoid activation function \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Sample solution for flattening images\n",
    "\n",
    "inputs = images.view(images.shape[0],-1) #change the size of the image iterator based on the first image and flatten\n",
    "\n",
    "#Create the weights and bias parameters\n",
    "#build network with 784 input units, 256 hidden units and 10 output units for weights and biases\n",
    "\n",
    "#Input layer\n",
    "w1 = torch.randn(784,256)\n",
    "b1 = torch.randn(256)\n",
    "\n",
    "#hidden layer\n",
    "w2 = torch.randn(256,10)\n",
    "b2 = torch.randn(10)\n",
    "\n",
    "#Connected layers\n",
    "h = activation(torch.mm(inputs,w1)+b1)\n",
    "out = torch.mm(h,w2)+b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "source": [
    "#Define the softmax activation function to obtain a probability distribution of the result\n",
    "\n",
    "def softmax(x):\n",
    "    return torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1, 1)\n",
    "\n",
    "probabilities = softmax(out)\n",
    "\n",
    "\n",
    "#Setting dim=0 takes the sum across the rows while dim=1 takes the sum across the columns.\n",
    "\n",
    "#=> Sanity checks\n",
    "\n",
    "# Does it have the right shape? Should be (64, 10)\n",
    "print(probabilities.shape)\n",
    "# Does it sum to 1?\n",
    "print(probabilities.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        # Output layer, 10 units - one for each digit\n",
    "        self.output = nn.Linear(256, 10)\n",
    "        \n",
    "        # Define sigmoid activation and softmax output \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        x = self.hidden(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through this bit by bit.\n",
    "\n",
    "```python\n",
    "class Network(nn.Module):\n",
    "```\n",
    "\n",
    "Here we're inheriting from `nn.Module`. Combined with `super().__init__()` this creates a class that tracks the architecture and provides a lot of useful methods and attributes. It is mandatory to inherit from `nn.Module` when you're creating a class for your network. The name of the class itself can be anything.\n",
    "\n",
    "```python\n",
    "self.hidden = nn.Linear(784, 256)\n",
    "```\n",
    "\n",
    "This line creates a module for a linear transformation, $x\\mathbf{W} + b$, with 784 inputs and 256 outputs and assigns it to `self.hidden`. The module automatically creates the weight and bias tensors which we'll use in the `forward` method. You can access the weight and bias tensors once the network (`net`) is created with `net.hidden.weight` and `net.hidden.bias`.\n",
    "\n",
    "```python\n",
    "self.output = nn.Linear(256, 10)\n",
    "```\n",
    "\n",
    "Similarly, this creates another linear transformation with 256 inputs and 10 outputs.\n",
    "\n",
    "```python\n",
    "self.sigmoid = nn.Sigmoid()\n",
    "self.softmax = nn.Softmax(dim=1)\n",
    "```\n",
    "\n",
    "Here I defined operations for the sigmoid activation and softmax output. Setting `dim=1` in `nn.Softmax(dim=1)` calculates softmax across the columns.\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "```\n",
    "\n",
    "PyTorch networks created with `nn.Module` must have a `forward` method defined. It takes in a tensor `x` and passes it through the operations you defined in the `__init__` method.\n",
    "\n",
    "```python\n",
    "x = self.hidden(x)\n",
    "x = self.sigmoid(x)\n",
    "x = self.output(x)\n",
    "x = self.softmax(x)\n",
    "```\n",
    "\n",
    "Here the input tensor `x` is passed through each operation a reassigned to `x`. We can see that the input tensor goes through the hidden layer, then a sigmoid function, then the output layer, and finally the softmax function. It doesn't matter what you name the variables here, as long as the inputs and outputs of the operations match the network architecture you want to build. The order in which you define things in the `__init__` method doesn't matter, but you'll need to sequence the operations correctly in the `forward` method.\n",
    "\n",
    "Now we can create a `Network` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Text version of model architecture\n",
    "\n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Common way to define model using PyTorch\n",
    "\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class Network(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         # Inputs to hidden layer linear transformation\n",
    "#         self.hidden = nn.Linear(784, 256)\n",
    "#         # Output layer, 10 units - one for each digit\n",
    "#         self.output = nn.Linear(256, 10)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # Hidden layer with sigmoid activation\n",
    "#         x = F.sigmoid(self.hidden(x))\n",
    "#         # Output layer with softmax activation\n",
    "#         x = F.softmax(self.output(x), dim=1)\n",
    "        \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Create a network with 784 input units, a hidden layer with 128 units and a ReLU activation, \n",
    "#then a hidden layer with 64 units and a ReLU activation, \n",
    "#and finally an output layer with a softmax activation as shown above.\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Defining the layers, 128, 64, 10 units each\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        # Output layer, 10 units - one for each digit\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' Forward pass through the network, returns the output logits '''\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = Network()\n",
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): Softmax()\n",
      ")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'helper' has no attribute 'view_classify'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-83db15e6dd44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m784\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mhelper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview_classify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'helper' has no attribute 'view_classify'"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import helper\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameters for our network\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Softmax(dim=1))\n",
    "print(model)\n",
    "\n",
    "# Forward pass through the network and display output\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "ps = model.forward(images[0,:])\n",
    "helper.view_classify(images[0].view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
